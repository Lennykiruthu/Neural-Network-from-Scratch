{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW2VMJ/ADc3x2NGrRUqejc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Z4QIymWCSLzF","executionInfo":{"status":"ok","timestamp":1747374214386,"user_tz":-180,"elapsed":9632,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["class NaiveDense:\n","  # Intialize variables\n","  def __init__(self, input_size, output_size, activation):\n","    # Create activation object linked to activation input\n","    self.activation = activation\n","\n","    '''\n","    Create the shape of the weights matrix where input_size corresponds to\n","    the number of features and output_size to the number of neurons.\n","    This shape enables matrix multiplication between the inputs and weights.\n","\n","    Initialize the weights using a uniform distribution, which samples values\n","    evenly between 0 and 0.1. This keeps initial weights small, helping to\n","    avoid exploding outputs or gradients during training.\n","\n","    Wrap the initialized weights in a TensorFlow Variable so they can be updated\n","    during training via backpropagation.\n","    '''\n","    w_shape        = (input_size, output_size)\n","    w_intial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n","    self.W         = tf.Variable(w_intial_value)\n","Iterate batch sizes sequentially\n","    '''\n","    Create the shape of the bias vector using output_size, as each neuron\n","    gets a single bias value.\n","\n","    Initialize all biases to zero and wrap them in a TensorFlow Variable\n","    so they can also be updated during training.\n","    '''\n","    b_shape        = (output_size,)\n","    b_intial_value = tf.zeros(b_shape)\n","    self.b         = tf.Variable(b_intial_value)\n","\n","  # Define how the layer is called on inputs — performs the forward pass\n","  def __call__(self, inputs):\n","    return self.activation(tf.matmul(inputs, self.W) + self.b)\n","\n","  # Provide a convenient way to access the layer's weights and biases\n","  # \\@property allows the functions to act similar to a variable\n","  @property\n","  def weights(self):\n","    return [self.W, self.b]\n"],"metadata":{"id":"aMbdiiUYpiPE","executionInfo":{"status":"ok","timestamp":1747374214400,"user_tz":-180,"elapsed":4,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class NaiveSequential:\n","  # Intialize thw self.layers object for further automation\n","  def __init__(self, layers):\n","    self.layers = layers\n","\n","  '''\n","  Defines how the model handles input data.\n","\n","  When the model is called with input data, it passes the data sequentially\n","  through each NaiveDense layer by invoking that layer’s __call__ method.\n","\n","  Each layer takes in the input, performs its own weight and bias\n","  computation (via NaiveDense.__call__), and passes the result to the next layer.\n","\n","  The final output after all layers is returned.\n","  '''\n","  def __call__(self, inputs):\n","    x = inputs\n","    for layer in self.layers:\n","      x = layer(x)\n","    return x\n","\n","  '''\n","  Collects and returns all the weights and biases from each NaiveDense layer.\n","\n","  Each NaiveDense instance has a `weights` property that returns a list\n","  containing its weight and bias tensors. This method gathers all such\n","  lists into a single flat list across all layers in the model.\n","  '''\n","  @property\n","  def weights(self):\n","    weights = []\n","    for layer in self.layers:\n","      weights += layer.weights\n","    return weights"],"metadata":{"id":"IpRfk8tIqEmz","executionInfo":{"status":"ok","timestamp":1747377439091,"user_tz":-180,"elapsed":26,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Call the sequential models this will create all the weights for all the layers.\n","model = NaiveSequential([\n","    NaiveDense(input_size = 28 * 28, output_size = 512, activation = tf.nn.relu),\n","    NaiveDense(input_size = 512, output_size = 10, activation = tf.nn.softmax)\n","])\n","\n","assert len(model.weights) == 4"],"metadata":{"id":"yT49eLeqzWI9","executionInfo":{"status":"ok","timestamp":1747378341666,"user_tz":-180,"elapsed":9,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import math\n","\n","class BatchGenerator:\n","  # Intialize relevant objects for calculating batch sizes\n","  def __init__(self, images, labels, batch_size=128):\n","    assert len(images) == len(labels)\n","    self.index      = 0\n","    self.images     = images\n","    self.labels     = labels\n","    self.batch_size = batch_size\n","    self.batch_num  = math.ceil(len(images) / self.batch_size)\n","\n","  # Return the next batch of images and labels\n","  def next(self):\n","    images = self.images[self.index : self.index+self.batch_size]\n","    labels = self.labels[self.index : self.index+self.batch_size]\n","    self.index += self.batch_size\n","    return images, labels"],"metadata":{"id":"XOu8pPlOz7ZP"},"execution_count":null,"outputs":[]}]}