{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMT1JPCMDZ3oWbt3/AVuEhV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Z4QIymWCSLzF","executionInfo":{"status":"ok","timestamp":1747716192537,"user_tz":-180,"elapsed":15476,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"outputs":[],"source":["import tensorflow as tf"]},{"cell_type":"code","source":["class NaiveDense:\n","  # Intialize variables\n","  def __init__(self, input_size, output_size, activation):\n","    # Create activation object linked to activation input\n","    self.activation = activation\n","\n","    '''\n","    Create the shape of the weights matrix where input_size corresponds to\n","    the number of features and output_size to the number of neurons.\n","    This shape enables matrix multiplication between the inputs and weights.\n","\n","    Initialize the weights using a uniform distribution, which samples values\n","    evenly between 0 and 0.1. This keeps initial weights small, helping to\n","    avoid exploding outputs or gradients during training.\n","\n","    Wrap the initialized weights in a TensorFlow Variable so they can be updated\n","    during training via backpropagation.\n","    '''\n","    w_shape        = (input_size, output_size)\n","    w_intial_value = tf.random.uniform(w_shape, minval=0, maxval=1e-1)\n","    self.W         = tf.Variable(w_intial_value)\n","\n","    '''\n","    Create the shape of the bias vector using output_size, as each neuron\n","    gets a single bias value.\n","\n","    Initialize all biases to zero and wrap them in a TensorFlow Variable\n","    so they can also be updated during training.\n","    '''\n","    b_shape        = (output_size,)\n","    b_intial_value = tf.zeros(b_shape)\n","    self.b         = tf.Variable(b_intial_value)\n","\n","  # Define how the layer is called on inputs — performs the forward pass\n","  def __call__(self, inputs):\n","    return self.activation(tf.matmul(inputs, self.W) + self.b)\n","\n","  # Provide a convenient way to access the layer's weights and biases\n","  # \\@property allows the functions to act similar to a variable\n","  @property\n","  def weights(self):\n","    return [self.W, self.b]\n"],"metadata":{"id":"aMbdiiUYpiPE","executionInfo":{"status":"ok","timestamp":1747716256203,"user_tz":-180,"elapsed":33,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class NaiveSequential:\n","  # Intialize thw self.layers object for further automation\n","  def __init__(self, layers):\n","    self.layers = layers\n","\n","  '''\n","  Defines how the model handles input data.\n","\n","  When the model is called with input data, it passes the data sequentially\n","  through each NaiveDense layer by invoking that layer’s __call__ method.\n","\n","  Each layer takes in the input, performs its own weight and bias\n","  computation (via NaiveDense.__call__), and passes the result to the next layer.\n","\n","  The final output after all layers is returned.\n","  '''\n","  def __call__(self, inputs):\n","    x = inputs\n","    for layer in self.layers:\n","      x = layer(x)\n","    return x\n","\n","  '''\n","  Collects and returns all the weights and biases from each NaiveDense layer.\n","\n","  Each NaiveDense instance has a `weights` property that returns a list\n","  containing its weight and bias tensors. This method gathers all such\n","  lists into a single flat list across all layers in the model.\n","  '''\n","  @property\n","  def weights(self):\n","    weights = []\n","    for layer in self.layers:\n","      weights += layer.weights\n","    return weights"],"metadata":{"id":"IpRfk8tIqEmz","executionInfo":{"status":"ok","timestamp":1747716260824,"user_tz":-180,"elapsed":6,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Call the sequential models this will create all the weights for all the layers.\n","model = NaiveSequential([\n","    NaiveDense(input_size = 28 * 28, output_size = 512, activation = tf.nn.relu),\n","    NaiveDense(input_size = 512, output_size = 10, activation = tf.nn.softmax)\n","])\n","\n","assert len(model.weights) == 4"],"metadata":{"id":"yT49eLeqzWI9","executionInfo":{"status":"ok","timestamp":1747716263940,"user_tz":-180,"elapsed":65,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import math\n","\n","'''\n","The BatchGenerator class splits the dataset into smaller, manageable batches\n","for training. This batching process is a common preprocessing step in machine\n","learning that enables efficient computation and improves training performance\n","by allowing multiple iterations (batches) over the data in each epoch.\n","'''\n","\n","class BatchGenerator:\n","  # Intialize relevant objects for calculating batch sizes\n","  def __init__(self, images, labels, batch_size=128):\n","    assert len(images) == len(labels)\n","    self.index      = 0\n","    self.images     = images\n","    self.labels     = labels\n","    self.batch_size = batch_size\n","    self.batch_num  = math.ceil(len(images) / self.batch_size)\n","\n","  # Return the next batch of images and labels\n","  def next(self):\n","    images = self.images[self.index : self.index+self.batch_size]\n","    labels = self.labels[self.index : self.index+self.batch_size]\n","    self.index += self.batch_size\n","    return images, labels"],"metadata":{"id":"XOu8pPlOz7ZP","executionInfo":{"status":"ok","timestamp":1747716265641,"user_tz":-180,"elapsed":38,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["'''\n","The `one_training_step` function performs a single forward and backward pass:\n","\n","1. It uses the model to make predictions on a batch of input images.\n","2. It computes the per-sample loss between the predicted and true labels using\n","   sparse categorical crossentropy.\n","3. It averages these losses to get a single scalar loss value representing\n","   how well the model performed on the entire batch.\n","4. It then calculates the gradients (partial derivatives) of this average loss\n","   with respect to each of the model's weights.\n","\n","These gradients indicate how much each weight contributed to the loss and\n","are used for updating the model.\n","\n","The `update_weights` function (defined elsewhere) applies these gradients\n","to the model's weights using an optimization algorithm such as gradient descent.\n","'''\n","\n","def one_training_step(model, images_batch, labels_batch):\n","  with tf.GradientTape() as tape:\n","    predictions       = model(images_batch)\n","    per_sample_losses = tf.keras.losses.sparse_categorical_crossentropy(labels_batch, predictions)\n","    average_loss      = tf.reduce_mean(per_sample_losses)\n","  gradients = tape.gradient(average_loss, model.weights)\n","  update_weights(gradients, model.weights)\n","  return average_loss"],"metadata":{"id":"Yxs8kMeiEtQv","executionInfo":{"status":"ok","timestamp":1747723801627,"user_tz":-180,"elapsed":46,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["'''\n","The update_weights function represents the final step of backpropagation.\n","It defines and applies the gradient descent update rule by adjusting each model weight\n","in the direction that minimizes the loss. This is done by subtracting the product of the\n","gradient and the learning rate from each corresponding weight.\n","'''\n","\n","learning_rate = 1e-3\n","\n","def update_weights(gradients, weights):\n","  for g,w in zip(gradients, weights):\n","    w.assign_sub(g * learning_rate)"],"metadata":{"id":"pK-G8flYOv8d","executionInfo":{"status":"ok","timestamp":1747722001677,"user_tz":-180,"elapsed":56,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["'''\n","The fit function performs the training loop. It iterates over the specified number\n","of epochs, and within each epoch, it loops through the dataset in batches.\n","For each batch, it trains the model and prints the loss at regular intervals.\n","'''\n","\n","\n","def fit(model, images, labels, epochs, batch_size=128):\n","  for epoch_counter in range(epochs):\n","    print(f'Epoch no. {epoch_counter}')\n","\n","    # Create an instance of the BatchGenerator class.\n","    batch_generator = BatchGenerator(images, labels)\n","    for batch_counter in range(batch_generator.batch_num):\n","      batch_images, batch_labels = batch_generator.next()\n","      loss = one_training_step(model, batch_images, batch_labels)\n","      if batch_counter % 100 == 0:\n","        print(f'loss at batch {batch_counter} is {loss:.2f}')"],"metadata":{"id":"i-znrwmzWVtu","executionInfo":{"status":"ok","timestamp":1747723768396,"user_tz":-180,"elapsed":22,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.datasets import mnist\n","\n","(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n","\n","train_images = train_images.reshape((60000, 28 * 28))\n","train_images = train_images.astype(\"float32\") / 255\n","test_images = test_images.reshape((10000, 28 * 28))\n","test_images = test_images.astype(\"float32\") / 255\n","\n","fit(model, train_images, train_labels, epochs=10, batch_size=128)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vF-x8MJYclYH","executionInfo":{"status":"ok","timestamp":1747723871168,"user_tz":-180,"elapsed":67089,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}},"outputId":"f46da6db-40dd-4fbf-e0d1-086d1fdd8efd"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch no. 0\n","loss at batch 0 is 3.87\n","loss at batch 100 is 2.22\n","loss at batch 200 is 2.17\n","loss at batch 300 is 2.08\n","loss at batch 400 is 2.23\n","Epoch no. 1\n","loss at batch 0 is 1.90\n","loss at batch 100 is 1.87\n","loss at batch 200 is 1.79\n","loss at batch 300 is 1.70\n","loss at batch 400 is 1.84\n","Epoch no. 2\n","loss at batch 0 is 1.57\n","loss at batch 100 is 1.57\n","loss at batch 200 is 1.47\n","loss at batch 300 is 1.42\n","loss at batch 400 is 1.52\n","Epoch no. 3\n","loss at batch 0 is 1.31\n","loss at batch 100 is 1.33\n","loss at batch 200 is 1.21\n","loss at batch 300 is 1.20\n","loss at batch 400 is 1.28\n","Epoch no. 4\n","loss at batch 0 is 1.11\n","loss at batch 100 is 1.15\n","loss at batch 200 is 1.02\n","loss at batch 300 is 1.04\n","loss at batch 400 is 1.11\n","Epoch no. 5\n","loss at batch 0 is 0.96\n","loss at batch 100 is 1.01\n","loss at batch 200 is 0.89\n","loss at batch 300 is 0.92\n","loss at batch 400 is 0.99\n","Epoch no. 6\n","loss at batch 0 is 0.86\n","loss at batch 100 is 0.91\n","loss at batch 200 is 0.79\n","loss at batch 300 is 0.83\n","loss at batch 400 is 0.90\n","Epoch no. 7\n","loss at batch 0 is 0.77\n","loss at batch 100 is 0.82\n","loss at batch 200 is 0.71\n","loss at batch 300 is 0.76\n","loss at batch 400 is 0.83\n","Epoch no. 8\n","loss at batch 0 is 0.71\n","loss at batch 100 is 0.76\n","loss at batch 200 is 0.65\n","loss at batch 300 is 0.71\n","loss at batch 400 is 0.78\n","Epoch no. 9\n","loss at batch 0 is 0.66\n","loss at batch 100 is 0.70\n","loss at batch 200 is 0.61\n","loss at batch 300 is 0.66\n","loss at batch 400 is 0.74\n"]}]},{"cell_type":"code","source":["import numpy as np\n","\n","predictions = model(test_images)\n","predictions = predictions.numpy()\n","predicted_labels = np.argmax(predictions, axis=1)\n","matches = predicted_labels == test_labels\n","print(f\"accuracy: {matches.mean():.2f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wdDlvqUddFjN","executionInfo":{"status":"ok","timestamp":1747724013248,"user_tz":-180,"elapsed":382,"user":{"displayName":"Lenny Kiruthu","userId":"12528909142045801793"}},"outputId":"b4f5b200-9675-4b96-f086-0bd010405cea"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy: 0.82\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9_kFSQeaeAu1"},"execution_count":null,"outputs":[]}]}